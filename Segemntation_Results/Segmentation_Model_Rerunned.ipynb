{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6673dff-fb82-4760-97f2-eab14383d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu118\n",
      "CUDA Available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab40612-4148-4ab2-aba0-de4329d71b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: Segmentation_Results\n",
      "Created directory: files\n",
      "Train: 2594 - 2594\n",
      "Valid: 100 - 100\n",
      "Test: 1000 - 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.3531, Train Acc: 0.8731, Val Loss: 0.2809, Val Dice: 0.7273, Val IoU: 0.5805, Val Acc: 0.8627\n",
      "Saved best model at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Train Loss: 0.2166, Train Acc: 0.9127, Val Loss: 0.2130, Val Dice: 0.7894, Val IoU: 0.6642, Val Acc: 0.8954\n",
      "Saved best model at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Train Loss: 0.1906, Train Acc: 0.9227, Val Loss: 0.2087, Val Dice: 0.7923, Val IoU: 0.6703, Val Acc: 0.9025\n",
      "Saved best model at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Train Loss: 0.1754, Train Acc: 0.9277, Val Loss: 0.1910, Val Dice: 0.8099, Val IoU: 0.6885, Val Acc: 0.9047\n",
      "Saved best model at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Train Loss: 0.1649, Train Acc: 0.9322, Val Loss: 0.1904, Val Dice: 0.8103, Val IoU: 0.6941, Val Acc: 0.9083\n",
      "Saved best model at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Train Loss: 0.1641, Train Acc: 0.9328, Val Loss: 0.1784, Val Dice: 0.8223, Val IoU: 0.7069, Val Acc: 0.9141\n",
      "Saved best model at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Train Loss: 0.1571, Train Acc: 0.9352, Val Loss: 0.1929, Val Dice: 0.8076, Val IoU: 0.6834, Val Acc: 0.9037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Train Loss: 0.1499, Train Acc: 0.9381, Val Loss: 0.1895, Val Dice: 0.8108, Val IoU: 0.6914, Val Acc: 0.9077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Train Loss: 0.1488, Train Acc: 0.9393, Val Loss: 0.1712, Val Dice: 0.8291, Val IoU: 0.7199, Val Acc: 0.9196\n",
      "Saved best model at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 0.1440, Train Acc: 0.9408, Val Loss: 0.1655, Val Dice: 0.8352, Val IoU: 0.7236, Val Acc: 0.9148\n",
      "Saved best model at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Train Loss: 0.1422, Train Acc: 0.9410, Val Loss: 0.1500, Val Dice: 0.8505, Val IoU: 0.7428, Val Acc: 0.9185\n",
      "Saved best model at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Train Loss: 0.1366, Train Acc: 0.9434, Val Loss: 0.1629, Val Dice: 0.8377, Val IoU: 0.7233, Val Acc: 0.9138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Train Loss: 0.1365, Train Acc: 0.9436, Val Loss: 0.1987, Val Dice: 0.8016, Val IoU: 0.6832, Val Acc: 0.9112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Train Loss: 0.1343, Train Acc: 0.9439, Val Loss: 0.1947, Val Dice: 0.8054, Val IoU: 0.6814, Val Acc: 0.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Train Loss: 0.1365, Train Acc: 0.9434, Val Loss: 0.1429, Val Dice: 0.8575, Val IoU: 0.7548, Val Acc: 0.9240\n",
      "Saved best model at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Train Loss: 0.1279, Train Acc: 0.9472, Val Loss: 0.1712, Val Dice: 0.8291, Val IoU: 0.7132, Val Acc: 0.9134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Train Loss: 0.1291, Train Acc: 0.9468, Val Loss: 0.1684, Val Dice: 0.8317, Val IoU: 0.7228, Val Acc: 0.9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Train Loss: 0.1316, Train Acc: 0.9449, Val Loss: 0.1626, Val Dice: 0.8377, Val IoU: 0.7293, Val Acc: 0.9202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Train Loss: 0.1333, Train Acc: 0.9449, Val Loss: 0.1667, Val Dice: 0.8336, Val IoU: 0.7203, Val Acc: 0.9133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Train Loss: 0.1231, Train Acc: 0.9490, Val Loss: 0.1276, Val Dice: 0.8727, Val IoU: 0.7765, Val Acc: 0.9318\n",
      "Saved best model at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Train Loss: 0.1217, Train Acc: 0.9494, Val Loss: 0.1607, Val Dice: 0.8395, Val IoU: 0.7296, Val Acc: 0.9153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Train Loss: 0.1236, Train Acc: 0.9492, Val Loss: 0.1480, Val Dice: 0.8522, Val IoU: 0.7538, Val Acc: 0.9297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Train Loss: 0.1174, Train Acc: 0.9512, Val Loss: 0.1716, Val Dice: 0.8286, Val IoU: 0.7163, Val Acc: 0.9159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Train Loss: 0.1194, Train Acc: 0.9509, Val Loss: 0.1267, Val Dice: 0.8735, Val IoU: 0.7772, Val Acc: 0.9306\n",
      "Saved best model at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Train Loss: 0.1178, Train Acc: 0.9504, Val Loss: 0.1449, Val Dice: 0.8553, Val IoU: 0.7561, Val Acc: 0.9293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Train Loss: 0.1148, Train Acc: 0.9520, Val Loss: 0.1238, Val Dice: 0.8764, Val IoU: 0.7838, Val Acc: 0.9357\n",
      "Saved best model at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Train Loss: 0.1136, Train Acc: 0.9531, Val Loss: 0.1331, Val Dice: 0.8670, Val IoU: 0.7669, Val Acc: 0.9264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Train Loss: 0.1126, Train Acc: 0.9534, Val Loss: 0.1339, Val Dice: 0.8663, Val IoU: 0.7706, Val Acc: 0.9331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Train Loss: 0.1093, Train Acc: 0.9546, Val Loss: 0.1201, Val Dice: 0.8800, Val IoU: 0.7884, Val Acc: 0.9362\n",
      "Saved best model at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Train Loss: 0.1119, Train Acc: 0.9532, Val Loss: 0.1311, Val Dice: 0.8691, Val IoU: 0.7738, Val Acc: 0.9328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Train Loss: 0.1108, Train Acc: 0.9533, Val Loss: 0.1546, Val Dice: 0.8455, Val IoU: 0.7421, Val Acc: 0.9261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Train Loss: 0.1151, Train Acc: 0.9517, Val Loss: 0.1431, Val Dice: 0.8570, Val IoU: 0.7600, Val Acc: 0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Train Loss: 0.1092, Train Acc: 0.9543, Val Loss: 0.1361, Val Dice: 0.8641, Val IoU: 0.7661, Val Acc: 0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, Train Loss: 0.1147, Train Acc: 0.9517, Val Loss: 0.1163, Val Dice: 0.8839, Val IoU: 0.7957, Val Acc: 0.9392\n",
      "Saved best model at epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Train Loss: 0.1136, Train Acc: 0.9525, Val Loss: 0.1652, Val Dice: 0.8349, Val IoU: 0.7223, Val Acc: 0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, Train Loss: 0.1161, Train Acc: 0.9516, Val Loss: 0.1371, Val Dice: 0.8631, Val IoU: 0.7632, Val Acc: 0.9280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Train Loss: 0.1096, Train Acc: 0.9544, Val Loss: 0.1391, Val Dice: 0.8610, Val IoU: 0.7595, Val Acc: 0.9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Train Loss: 0.1076, Train Acc: 0.9553, Val Loss: 0.1269, Val Dice: 0.8732, Val IoU: 0.7812, Val Acc: 0.9366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100, Train Loss: 0.1063, Train Acc: 0.9559, Val Loss: 0.1217, Val Dice: 0.8785, Val IoU: 0.7869, Val Acc: 0.9365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Train Loss: 0.1058, Train Acc: 0.9560, Val Loss: 0.1299, Val Dice: 0.8702, Val IoU: 0.7777, Val Acc: 0.9364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Train Loss: 0.1055, Train Acc: 0.9562, Val Loss: 0.1263, Val Dice: 0.8738, Val IoU: 0.7794, Val Acc: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Train Loss: 0.1009, Train Acc: 0.9574, Val Loss: 0.1243, Val Dice: 0.8758, Val IoU: 0.7815, Val Acc: 0.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Train Loss: 0.1022, Train Acc: 0.9573, Val Loss: 0.1202, Val Dice: 0.8799, Val IoU: 0.7874, Val Acc: 0.9347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Train Loss: 0.1038, Train Acc: 0.9565, Val Loss: 0.1179, Val Dice: 0.8822, Val IoU: 0.7944, Val Acc: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Train Loss: 0.1007, Train Acc: 0.9577, Val Loss: 0.1636, Val Dice: 0.8365, Val IoU: 0.7309, Val Acc: 0.9245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, Train Loss: 0.1050, Train Acc: 0.9558, Val Loss: 0.1226, Val Dice: 0.8775, Val IoU: 0.7881, Val Acc: 0.9385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100, Train Loss: 0.1015, Train Acc: 0.9576, Val Loss: 0.1248, Val Dice: 0.8753, Val IoU: 0.7838, Val Acc: 0.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100, Train Loss: 0.0977, Train Acc: 0.9586, Val Loss: 0.1105, Val Dice: 0.8895, Val IoU: 0.8037, Val Acc: 0.9417\n",
      "Saved best model at epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Train Loss: 0.0985, Train Acc: 0.9585, Val Loss: 0.1337, Val Dice: 0.8664, Val IoU: 0.7663, Val Acc: 0.9256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Train Loss: 0.0988, Train Acc: 0.9586, Val Loss: 0.1288, Val Dice: 0.8713, Val IoU: 0.7768, Val Acc: 0.9338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100, Train Loss: 0.1010, Train Acc: 0.9577, Val Loss: 0.1242, Val Dice: 0.8759, Val IoU: 0.7850, Val Acc: 0.9374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100, Train Loss: 0.0976, Train Acc: 0.9590, Val Loss: 0.1119, Val Dice: 0.8882, Val IoU: 0.8011, Val Acc: 0.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Train Loss: 0.0972, Train Acc: 0.9590, Val Loss: 0.1099, Val Dice: 0.8903, Val IoU: 0.8046, Val Acc: 0.9422\n",
      "Saved best model at epoch 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Train Loss: 0.0950, Train Acc: 0.9599, Val Loss: 0.1058, Val Dice: 0.8942, Val IoU: 0.8102, Val Acc: 0.9434\n",
      "Saved best model at epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100, Train Loss: 0.0950, Train Acc: 0.9597, Val Loss: 0.1166, Val Dice: 0.8835, Val IoU: 0.7933, Val Acc: 0.9373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Train Loss: 0.0914, Train Acc: 0.9612, Val Loss: 0.1096, Val Dice: 0.8904, Val IoU: 0.8052, Val Acc: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Train Loss: 0.0987, Train Acc: 0.9589, Val Loss: 0.1666, Val Dice: 0.8335, Val IoU: 0.7163, Val Acc: 0.8978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100, Train Loss: 0.1011, Train Acc: 0.9573, Val Loss: 0.1214, Val Dice: 0.8787, Val IoU: 0.7862, Val Acc: 0.9331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100, Train Loss: 0.0958, Train Acc: 0.9601, Val Loss: 0.1214, Val Dice: 0.8787, Val IoU: 0.7860, Val Acc: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Train Loss: 0.0919, Train Acc: 0.9616, Val Loss: 0.1187, Val Dice: 0.8814, Val IoU: 0.7909, Val Acc: 0.9354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100, Train Loss: 0.0886, Train Acc: 0.9627, Val Loss: 0.1232, Val Dice: 0.8768, Val IoU: 0.7874, Val Acc: 0.9387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100, Train Loss: 0.0949, Train Acc: 0.9604, Val Loss: 0.1160, Val Dice: 0.8841, Val IoU: 0.7948, Val Acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100, Train Loss: 0.0910, Train Acc: 0.9619, Val Loss: 0.1174, Val Dice: 0.8827, Val IoU: 0.7937, Val Acc: 0.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100, Train Loss: 0.0918, Train Acc: 0.9611, Val Loss: 0.1224, Val Dice: 0.8777, Val IoU: 0.7849, Val Acc: 0.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Train Loss: 0.0934, Train Acc: 0.9607, Val Loss: 0.1114, Val Dice: 0.8887, Val IoU: 0.8034, Val Acc: 0.9429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100, Train Loss: 0.0937, Train Acc: 0.9604, Val Loss: 0.1896, Val Dice: 0.8104, Val IoU: 0.6933, Val Acc: 0.9150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100, Train Loss: 0.0980, Train Acc: 0.9588, Val Loss: 0.1533, Val Dice: 0.8467, Val IoU: 0.7416, Val Acc: 0.9264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100, Train Loss: 0.0991, Train Acc: 0.9594, Val Loss: 0.1097, Val Dice: 0.8904, Val IoU: 0.8063, Val Acc: 0.9422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Train Loss: 0.0880, Train Acc: 0.9629, Val Loss: 0.1086, Val Dice: 0.8915, Val IoU: 0.8074, Val Acc: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Train Loss: 0.0839, Train Acc: 0.9643, Val Loss: 0.1174, Val Dice: 0.8827, Val IoU: 0.7948, Val Acc: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100, Train Loss: 0.0845, Train Acc: 0.9643, Val Loss: 0.1177, Val Dice: 0.8824, Val IoU: 0.7943, Val Acc: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100, Train Loss: 0.0878, Train Acc: 0.9637, Val Loss: 0.1229, Val Dice: 0.8771, Val IoU: 0.7861, Val Acc: 0.9361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Train Loss: 0.0915, Train Acc: 0.9617, Val Loss: 0.1110, Val Dice: 0.8890, Val IoU: 0.8024, Val Acc: 0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Train Loss: 0.0913, Train Acc: 0.9613, Val Loss: 0.1186, Val Dice: 0.8814, Val IoU: 0.7921, Val Acc: 0.9370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Train Loss: 0.0922, Train Acc: 0.9610, Val Loss: 0.1235, Val Dice: 0.8766, Val IoU: 0.7825, Val Acc: 0.9331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Train Loss: 0.0933, Train Acc: 0.9609, Val Loss: 0.1107, Val Dice: 0.8893, Val IoU: 0.8041, Val Acc: 0.9417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Train Loss: 0.0831, Train Acc: 0.9646, Val Loss: 0.1261, Val Dice: 0.8739, Val IoU: 0.7779, Val Acc: 0.9304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Train Loss: 0.0867, Train Acc: 0.9636, Val Loss: 0.1177, Val Dice: 0.8824, Val IoU: 0.7955, Val Acc: 0.9397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100, Train Loss: 0.0829, Train Acc: 0.9651, Val Loss: 0.1083, Val Dice: 0.8918, Val IoU: 0.8087, Val Acc: 0.9440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Train Loss: 0.0835, Train Acc: 0.9645, Val Loss: 0.1504, Val Dice: 0.8497, Val IoU: 0.7462, Val Acc: 0.9284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Train Loss: 0.0878, Train Acc: 0.9632, Val Loss: 0.1146, Val Dice: 0.8855, Val IoU: 0.7976, Val Acc: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Train Loss: 0.0869, Train Acc: 0.9631, Val Loss: 0.1122, Val Dice: 0.8878, Val IoU: 0.8030, Val Acc: 0.9409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100, Train Loss: 0.0832, Train Acc: 0.9648, Val Loss: 0.1171, Val Dice: 0.8830, Val IoU: 0.7948, Val Acc: 0.9392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100, Train Loss: 0.0834, Train Acc: 0.9647, Val Loss: 0.1315, Val Dice: 0.8685, Val IoU: 0.7713, Val Acc: 0.9338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Train Loss: 0.0881, Train Acc: 0.9628, Val Loss: 0.1211, Val Dice: 0.8790, Val IoU: 0.7898, Val Acc: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100, Train Loss: 0.0819, Train Acc: 0.9655, Val Loss: 0.1260, Val Dice: 0.8740, Val IoU: 0.7790, Val Acc: 0.9320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Train Loss: 0.0807, Train Acc: 0.9654, Val Loss: 0.1156, Val Dice: 0.8844, Val IoU: 0.7967, Val Acc: 0.9396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100, Train Loss: 0.0818, Train Acc: 0.9654, Val Loss: 0.1075, Val Dice: 0.8925, Val IoU: 0.8097, Val Acc: 0.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Train Loss: 0.0804, Train Acc: 0.9662, Val Loss: 0.1607, Val Dice: 0.8394, Val IoU: 0.7366, Val Acc: 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Train Loss: 0.0884, Train Acc: 0.9629, Val Loss: 0.1181, Val Dice: 0.8819, Val IoU: 0.7917, Val Acc: 0.9369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100, Train Loss: 0.0824, Train Acc: 0.9651, Val Loss: 0.1308, Val Dice: 0.8693, Val IoU: 0.7710, Val Acc: 0.9251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100, Train Loss: 0.0879, Train Acc: 0.9632, Val Loss: 0.1094, Val Dice: 0.8907, Val IoU: 0.8073, Val Acc: 0.9417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100, Train Loss: 0.0803, Train Acc: 0.9659, Val Loss: 0.1143, Val Dice: 0.8857, Val IoU: 0.7982, Val Acc: 0.9387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100, Train Loss: 0.0774, Train Acc: 0.9673, Val Loss: 0.1217, Val Dice: 0.8784, Val IoU: 0.7870, Val Acc: 0.9360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100, Train Loss: 0.1065, Train Acc: 0.9552, Val Loss: 0.1365, Val Dice: 0.8635, Val IoU: 0.7648, Val Acc: 0.9299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100, Train Loss: 0.0897, Train Acc: 0.9623, Val Loss: 0.1275, Val Dice: 0.8725, Val IoU: 0.7773, Val Acc: 0.9332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100, Train Loss: 0.0856, Train Acc: 0.9640, Val Loss: 0.1186, Val Dice: 0.8814, Val IoU: 0.7923, Val Acc: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100, Train Loss: 0.0820, Train Acc: 0.9656, Val Loss: 0.1066, Val Dice: 0.8934, Val IoU: 0.8094, Val Acc: 0.9433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Train Loss: 0.0788, Train Acc: 0.9667, Val Loss: 0.1201, Val Dice: 0.8800, Val IoU: 0.7907, Val Acc: 0.9398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Train Loss: 0.0806, Train Acc: 0.9663, Val Loss: 0.1109, Val Dice: 0.8891, Val IoU: 0.8047, Val Acc: 0.9420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 406\u001b[0m\n\u001b[0;32m    404\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(test_predictions)\n\u001b[0;32m    405\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(test_labels)\n\u001b[1;32m--> 406\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(test_labels, test_predictions, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    407\u001b[0m tn, fp, fn, tp \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:319\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    225\u001b[0m     {\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    236\u001b[0m ):\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:94\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     96\u001b[0m             type_true, type_pred\n\u001b[0;32m     97\u001b[0m         )\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    101\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Data directories\n",
    "train_image_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/Isic_2018_training_Images_Cleaned_v1\"\n",
    "train_mask_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/Isic_2018_training_ground_truth_V1\"\n",
    "val_image_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/Isic_2018_validation_Images_Cleaned_v1\"\n",
    "val_mask_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/Isic_2018_validation_ground_truth_V1\"\n",
    "test_image_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/Isic_2018_test_Images_Cleaned_v1\"\n",
    "test_mask_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/Isic_2018_test_ground_truth_V1\"\n",
    "\n",
    "# Global parameters\n",
    "H = 256\n",
    "W = 256\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# SE Block Definition\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# BatchNormReLU\n",
    "class BatchNormReLU(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(BatchNormReLU, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Residual Block with SE\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, strides=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=strides, padding=1, bias=False)\n",
    "        self.bn_relu1 = BatchNormReLU(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_relu2 = BatchNormReLU(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if strides != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=strides, padding=0, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn_relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn_relu2(x)\n",
    "        x = self.se(x)\n",
    "        x = x + identity\n",
    "        return x\n",
    "\n",
    "# Decoder Block with SE\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.concat_channels = in_channels + skip_channels\n",
    "        self.res_block = ResidualBlock(self.concat_channels, out_channels, strides=1)\n",
    "        self.se = SEBlock(out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res_block(x)\n",
    "        x = self.se(x)\n",
    "        return x\n",
    "\n",
    "# Dense PPM Bridge\n",
    "class DensePPMBridge(nn.Module):\n",
    "    def __init__(self, in_channels=256, out_channels=512):\n",
    "        super(DensePPMBridge, self).__init__()\n",
    "        growth_rate = 128\n",
    "\n",
    "        self.dense1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            BatchNormReLU(growth_rate)\n",
    "        )\n",
    "        self.dense2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            BatchNormReLU(growth_rate)\n",
    "        )\n",
    "\n",
    "        self.ppm_pool1 = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        self.ppm_pool2 = nn.Sequential(nn.AdaptiveAvgPool2d(2), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        self.ppm_pool3 = nn.Sequential(nn.AdaptiveAvgPool2d(3), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        self.ppm_pool6 = nn.Sequential(nn.AdaptiveAvgPool2d(6), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels + 2 * growth_rate + 4 * 128, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.dense1(x)\n",
    "        d1_cat = torch.cat([F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=True), d1], dim=1)\n",
    "        d2 = self.dense2(d1_cat)\n",
    "        dense_out = torch.cat([F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=True), d1, d2], dim=1)\n",
    "\n",
    "        ppm1 = self.ppm_pool1(dense_out)\n",
    "        ppm1 = F.interpolate(ppm1, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "        ppm2 = self.ppm_pool2(dense_out)\n",
    "        ppm2 = F.interpolate(ppm2, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "        ppm3 = self.ppm_pool3(dense_out)\n",
    "        ppm3 = F.interpolate(ppm3, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "        ppm6 = self.ppm_pool6(dense_out)\n",
    "        ppm6 = F.interpolate(ppm6, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        ppm_out = torch.cat([dense_out, ppm1, ppm2, ppm3, ppm6], dim=1)\n",
    "        out = self.final_conv(ppm_out)\n",
    "        return out\n",
    "\n",
    "# SE-ResUNet Model\n",
    "class SEResUNet(nn.Module):\n",
    "    def __init__(self, input_shape=(256, 256, 3)):\n",
    "        super(SEResUNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_relu1 = BatchNormReLU(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.se1 = SEBlock(64)\n",
    "        self.shortcut1 = nn.Conv2d(3, 64, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        self.encoder2 = ResidualBlock(64, 128, strides=2)\n",
    "        self.encoder3 = ResidualBlock(128, 256, strides=2)\n",
    "        self.bridge = DensePPMBridge(256, 512)\n",
    "\n",
    "        self.decoder1 = DecoderBlock(512, 256, 256)\n",
    "        self.decoder2 = DecoderBlock(256, 128, 128)\n",
    "        self.decoder3 = DecoderBlock(128, 64, 64)\n",
    "\n",
    "        self.output = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1 = self.conv1(x)\n",
    "        s1 = self.bn_relu1(s1)\n",
    "        s1 = self.conv2(s1)\n",
    "        s1 = self.se1(s1)\n",
    "        shortcut = self.shortcut1(x)\n",
    "        s1 = s1 + shortcut\n",
    "\n",
    "        s2 = self.encoder2(s1)\n",
    "        s3 = self.encoder3(s2)\n",
    "        b = self.bridge(s3)\n",
    "\n",
    "        d1 = self.decoder1(b, s3)\n",
    "        d2 = self.decoder2(d1, s2)\n",
    "        d3 = self.decoder3(d2, s1)\n",
    "\n",
    "        out = self.output(d3)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Metrics and Utility Functions\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-15):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = y_pred.view(-1)\n",
    "        y_true = y_true.view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
    "        return 1.0 - dice\n",
    "\n",
    "def dice_coef(y_pred, y_true, smooth=1e-15):\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_true = y_true.view(-1)\n",
    "    intersection = (y_pred * y_true).sum()\n",
    "    return (2. * intersection + smooth) / (y_pred.sum() + y_true.sum() + smooth)\n",
    "\n",
    "def iou(y_pred, y_true, smooth=1e-15):\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_true = y_true.view(-1)\n",
    "    intersection = (y_pred * y_true).sum()\n",
    "    union = y_true.sum() + y_pred.sum() - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_true = y_true.view(-1)\n",
    "    correct = (y_pred == y_true).float().sum()\n",
    "    total = y_true.numel()\n",
    "    return correct / total\n",
    "\n",
    "def create_dir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(f\"Created directory: {path}\")\n",
    "        else:\n",
    "            print(f\"Directory already exists: {path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_data():\n",
    "    train_x = sorted(glob(os.path.join(train_image_dir, \"*.jpg\")))\n",
    "    train_y = sorted(glob(os.path.join(train_mask_dir, \"*.png\")))\n",
    "    valid_x = sorted(glob(os.path.join(val_image_dir, \"*.jpg\")))\n",
    "    valid_y = sorted(glob(os.path.join(val_mask_dir, \"*.png\")))\n",
    "    test_x = sorted(glob(os.path.join(test_image_dir, \"*.jpg\")))\n",
    "    test_y = sorted(glob(os.path.join(test_mask_dir, \"*.png\")))\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img, (W, H))\n",
    "        img = img / 255.0\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        mask_path = self.masks[idx]\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, (W, H))\n",
    "        mask = mask / 255.0\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        return torch.tensor(img, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "# Main Execution\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "create_dir(\"Segmentation_Results\")\n",
    "create_dir(\"files\")\n",
    "\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data()\n",
    "\n",
    "print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
    "print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "train_dataset = ISICDataset(train_x, train_y)\n",
    "valid_dataset = ISICDataset(valid_x, valid_y)\n",
    "test_dataset = ISICDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = SEResUNet(input_shape=(H, W, 3)).to(device)\n",
    "criterion = DiceLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "model_path = os.path.join(\"files\", \"segModel.pth\")\n",
    "history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_dice': [],\n",
    "    'val_iou': [],\n",
    "    'val_acc': [],\n",
    "    'train_time': [],\n",
    "    'val_time': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_start_time = time.time()\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "    for images, masks in train_bar:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        outputs_bin = (outputs > 0.5).float()\n",
    "        train_acc += accuracy(outputs_bin, masks).item() * images.size(0)\n",
    "        train_bar.set_postfix({\"Train Loss\": loss.item(), \"Train Acc\": train_acc / (train_loss + 1e-10)})\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    train_time = (time.time() - train_start_time) * 1000 / len(train_loader)  # ms per batch\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_dice = 0.0\n",
    "    val_iou = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_start_time = time.time()\n",
    "    val_bar = tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_bar:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            outputs_bin = (outputs > 0.5).float()\n",
    "            val_dice += dice_coef(outputs_bin, masks).item() * images.size(0)\n",
    "            val_iou += iou(outputs_bin, masks).item() * images.size(0)\n",
    "            val_acc += accuracy(outputs_bin, masks).item() * images.size(0)\n",
    "            val_bar.set_postfix({\"Val Loss\": loss.item()})\n",
    "    \n",
    "    val_loss /= len(valid_loader.dataset)\n",
    "    val_dice /= len(valid_loader.dataset)\n",
    "    val_iou /= len(valid_loader.dataset)\n",
    "    val_acc /= len(valid_loader.dataset)\n",
    "    val_time = (time.time() - val_start_time) * 1000 / len(valid_loader)  # ms per batch\n",
    "\n",
    "    # Save history\n",
    "    history['epoch'].append(epoch + 1)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['train_time'].append(train_time)\n",
    "    history['val_time'].append(val_time)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val IoU: {val_iou:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d916a0f-e8d7-47cd-89f7-e7634111f9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics:\n",
      "Test Loss: 0.1408\n",
      "Test Dice: 0.8593\n",
      "Test IoU: 0.7589\n",
      "Test Accuracy: 0.9220\n",
      "\n",
      "Confusion Matrix:\n",
      "True Negatives: 44956796\n",
      "False Positives: 2292415\n",
      "False Negatives: 2772513\n",
      "True Positives: 15514276\n",
      "\n",
      "Metrics table saved to Segmentation_Results/metrics_table.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluate on test dataset\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_dice = 0.0\n",
    "test_iou = 0.0\n",
    "test_acc = 0.0\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "test_start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "    for images, masks in test_bar:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        outputs_bin = (outputs > 0.5).float()  # Binarize predictions\n",
    "        test_dice += dice_coef(outputs_bin, masks).item() * images.size(0)\n",
    "        test_iou += iou(outputs_bin, masks).item() * images.size(0)\n",
    "        test_acc += accuracy(outputs_bin, masks).item() * images.size(0)\n",
    "        test_predictions.append(outputs_bin.cpu().numpy().flatten().astype(np.int32))  # Convert to binary integers\n",
    "        test_labels.append(masks.cpu().numpy().flatten().astype(np.int32))  # Convert to binary integers\n",
    "        test_bar.set_postfix({\"Test Loss\": loss.item()})\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_dice /= len(test_loader.dataset)\n",
    "test_iou /= len(test_loader.dataset)\n",
    "test_acc /= len(test_loader.dataset)\n",
    "test_time = (time.time() - test_start_time) * 1000 / len(test_loader)  # ms per batch\n",
    "\n",
    "# Compute confusion matrix\n",
    "test_predictions = np.concatenate(test_predictions)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "cm = confusion_matrix(test_labels, test_predictions, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Dice: {test_dice:.4f}\")\n",
    "print(f\"Test IoU: {test_iou:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives: {tp}\")\n",
    "\n",
    "# Generate plots\n",
    "# Plot 1: Training and Validation Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['epoch'], history['train_loss'], label='Training Loss')\n",
    "plt.plot(history['epoch'], history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(\"Segmentation_Results\", \"loss_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Plot 2: Validation Dice and IoU\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['epoch'], history['val_dice'], label='Validation Dice')\n",
    "plt.plot(history['epoch'], history['val_iou'], label='Validation IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Validation Dice and IoU Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(\"Segmentation_Results\", \"dice_iou_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Plot 3: Training and Validation Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['epoch'], history['train_acc'], label='Training Accuracy')\n",
    "plt.plot(history['epoch'], history['val_acc'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(\"Segmentation_Results\", \"accuracy_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Plot 4: Sample Test Predictions\n",
    "num_samples = 3\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
    "axes[0, 0].set_title(\"Input Image\")\n",
    "axes[0, 1].set_title(\"Ground Truth\")\n",
    "axes[0, 2].set_title(\"Predicted Mask\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in enumerate(test_loader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        outputs_bin = (outputs > 0.5).float()\n",
    "        \n",
    "        img = images[0].cpu().numpy().transpose(1, 2, 0) * 255.0\n",
    "        mask = masks[0][0].cpu().numpy() * 255.0\n",
    "        pred = outputs_bin[0][0].cpu().numpy() * 255.0\n",
    "\n",
    "        axes[i, 0].imshow(img.astype(np.uint8))\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 1].imshow(mask, cmap='gray')\n",
    "        axes[i, 1].axis('off')\n",
    "        axes[i, 2].imshow(pred, cmap='gray')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"Segmentation_Results\", \"test_predictions.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Generate metrics table\n",
    "metrics_table = []\n",
    "for epoch in range(num_epochs):\n",
    "    test_acc_epoch = test_acc  # Use final test accuracy for all epochs\n",
    "    val_acc_epoch = history['val_acc'][epoch]\n",
    "    train_acc_epoch = history['train_acc'][epoch]\n",
    "    abs_variance = abs(test_acc_epoch - val_acc_epoch)\n",
    "    time_per_step = (history['train_time'][epoch] + history['val_time'][epoch]) / 2  # Average of train and val\n",
    "    metrics_table.append({\n",
    "        'Epoch': epoch + 1,\n",
    "        'Test Accuracy (%)': test_acc_epoch * 100,\n",
    "        'Validation Accuracy (%)': val_acc_epoch * 100,\n",
    "        'Training Accuracy (%)': train_acc_epoch * 100,\n",
    "        'Absolute Variance': abs_variance * 100,\n",
    "        'Time per Step (ms/step)': time_per_step\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_table)\n",
    "metrics_df.to_csv(os.path.join(\"Segmentation_Results\", \"metrics_table.csv\"), index=False)\n",
    "print(\"\\nMetrics table saved to Segmentation_Results/metrics_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32359c80-8fb2-4e45-970f-c642e54bc2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Training_Pseudo_Masks\n",
      "Created directory: E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Test_Pseudo_Masks\n",
      "Training images: 25331\n",
      "Test images: 8238\n",
      "Loaded model from files/segModel.pth\n",
      "Generating pseudo masks for training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo masks for E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Training_Pseudo_Masks: 100%|| 1584/1584 [07:3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pseudo masks for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo masks for E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Test_Pseudo_Masks: 100%|| 515/515 [02:36<00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo mask generation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data directories\n",
    "train_image_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Training_Input_Cleaned_Images\"\n",
    "test_image_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Test_Input_Cleaned_Images\"\n",
    "train_pseudo_mask_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Training_Pseudo_Masks\"\n",
    "test_pseudo_mask_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Test_Pseudo_Masks\"\n",
    "model_path = \"files/segModel.pth\"\n",
    "\n",
    "# Global parameters\n",
    "H = 256\n",
    "W = 256\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# SE Block Definition\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# BatchNormReLU\n",
    "class BatchNormReLU(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(BatchNormReLU, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Residual Block with SE\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, strides=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=strides, padding=1, bias=False)\n",
    "        self.bn_relu1 = BatchNormReLU(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_relu2 = BatchNormReLU(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if strides != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=strides, padding=0, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn_relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn_relu2(x)\n",
    "        x = self.se(x)\n",
    "        x = x + identity\n",
    "        return x\n",
    "\n",
    "# Decoder Block with SE\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.concat_channels = in_channels + skip_channels\n",
    "        self.res_block = ResidualBlock(self.concat_channels, out_channels, strides=1)\n",
    "        self.se = SEBlock(out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res_block(x)\n",
    "        x = self.se(x)\n",
    "        return x\n",
    "\n",
    "# Dense PPM Bridge\n",
    "class DensePPMBridge(nn.Module):\n",
    "    def __init__(self, in_channels=256, out_channels=512):\n",
    "        super(DensePPMBridge, self).__init__()\n",
    "        growth_rate = 128\n",
    "\n",
    "        self.dense1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            BatchNormReLU(growth_rate)\n",
    "        )\n",
    "        self.dense2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            BatchNormReLU(growth_rate)\n",
    "        )\n",
    "\n",
    "        self.ppm_pool1 = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        self.ppm_pool2 = nn.Sequential(nn.AdaptiveAvgPool2d(2), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        self.ppm_pool3 = nn.Sequential(nn.AdaptiveAvgPool2d(3), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        self.ppm_pool6 = nn.Sequential(nn.AdaptiveAvgPool2d(6), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels + 2 * growth_rate + 4 * 128, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.dense1(x)\n",
    "        d1_cat = torch.cat([F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=True), d1], dim=1)\n",
    "        d2 = self.dense2(d1_cat)\n",
    "        dense_out = torch.cat([F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=True), d1, d2], dim=1)\n",
    "\n",
    "        ppm1 = self.ppm_pool1(dense_out)\n",
    "        ppm1 = F.interpolate(ppm1, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "        ppm2 = self.ppm_pool2(dense_out)\n",
    "        ppm2 = F.interpolate(ppm2, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "        ppm3 = self.ppm_pool3(dense_out)\n",
    "        ppm3 = F.interpolate(ppm3, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "        ppm6 = self.ppm_pool6(dense_out)\n",
    "        ppm6 = F.interpolate(ppm6, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        ppm_out = torch.cat([dense_out, ppm1, ppm2, ppm3, ppm6], dim=1)\n",
    "        out = self.final_conv(ppm_out)\n",
    "        return out\n",
    "\n",
    "# SE-ResUNet Model\n",
    "class SEResUNet(nn.Module):\n",
    "    def __init__(self, input_shape=(256, 256, 3)):\n",
    "        super(SEResUNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_relu1 = BatchNormReLU(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.se1 = SEBlock(64)\n",
    "        self.shortcut1 = nn.Conv2d(3, 64, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        self.encoder2 = ResidualBlock(64, 128, strides=2)\n",
    "        self.encoder3 = ResidualBlock(128, 256, strides=2)\n",
    "        self.bridge = DensePPMBridge(256, 512)\n",
    "\n",
    "        self.decoder1 = DecoderBlock(512, 256, 256)\n",
    "        self.decoder2 = DecoderBlock(256, 128, 128)\n",
    "        self.decoder3 = DecoderBlock(128, 64, 64)\n",
    "\n",
    "        self.output = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1 = self.conv1(x)\n",
    "        s1 = self.bn_relu1(s1)\n",
    "        s1 = self.conv2(s1)\n",
    "        s1 = self.se1(s1)\n",
    "        shortcut = self.shortcut1(x)\n",
    "        s1 = s1 + shortcut\n",
    "\n",
    "        s2 = self.encoder2(s1)\n",
    "        s3 = self.encoder3(s2)\n",
    "        b = self.bridge(s3)\n",
    "\n",
    "        d1 = self.decoder1(b, s3)\n",
    "        d2 = self.decoder2(d1, s2)\n",
    "        d3 = self.decoder3(d2, s1)\n",
    "\n",
    "        out = self.output(d3)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Dataset for loading images (no masks needed for pseudo mask generation)\n",
    "class ISICImageDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img, (W, H))\n",
    "        img = img / 255.0\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        return torch.tensor(img, dtype=torch.float32), img_path\n",
    "\n",
    "# Utility function to create directories\n",
    "def create_dir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(f\"Created directory: {path}\")\n",
    "        else:\n",
    "            print(f\"Directory already exists: {path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load image paths\n",
    "def load_image_paths():\n",
    "    train_images = sorted(glob(os.path.join(train_image_dir, \"*.jpg\")))\n",
    "    test_images = sorted(glob(os.path.join(test_image_dir, \"*.jpg\")))\n",
    "    return train_images, test_images\n",
    "\n",
    "# Generate and save pseudo masks\n",
    "def generate_pseudo_masks(model, image_loader, output_dir):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, img_paths in tqdm(image_loader, desc=f\"Generating pseudo masks for {output_dir}\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs_bin = (outputs > 0.5).float()  # Binarize predictions\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                pred_mask = outputs_bin[i, 0].cpu().numpy() * 255.0  # Scale to 0-255 for saving as image\n",
    "                pred_mask = pred_mask.astype(np.uint8)\n",
    "                img_name = os.path.basename(img_paths[i]).replace('.jpg', '.png')\n",
    "                output_path = os.path.join(output_dir, img_name)\n",
    "                cv2.imwrite(output_path, pred_mask)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Create output directories\n",
    "    create_dir(train_pseudo_mask_dir)\n",
    "    create_dir(test_pseudo_mask_dir)\n",
    "\n",
    "    # Load image paths\n",
    "    train_images, test_images = load_image_paths()\n",
    "    print(f\"Training images: {len(train_images)}\")\n",
    "    print(f\"Test images: {len(test_images)}\")\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ISICImageDataset(train_images)\n",
    "    test_dataset = ISICImageDataset(test_images)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Load model\n",
    "    model = SEResUNet(input_shape=(H, W, 3)).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "    # Generate pseudo masks\n",
    "    print(\"Generating pseudo masks for training set...\")\n",
    "    generate_pseudo_masks(model, train_loader, train_pseudo_mask_dir)\n",
    "    print(\"Generating pseudo masks for test set...\")\n",
    "    generate_pseudo_masks(model, test_loader, test_pseudo_mask_dir)\n",
    "    print(\"Pseudo mask generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e94490b5-8ac5-4487-8615-a54904aae576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth loaded successfully. Columns: ['image', 'diagnosis']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|| 25331/25331 [08:25<00:00, 50.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE-balanced CSV saved to: E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Training_GroundTruth_SMOTE.csv\n",
      "Class distribution after SMOTE:\n",
      "diagnosis\n",
      "NV      12875\n",
      "MEL     12875\n",
      "BKL     12875\n",
      "DF      12875\n",
      "SCC     12875\n",
      "BCC     12875\n",
      "VASC    12875\n",
      "AK      12875\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Paths\n",
    "train_image_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Training_Input_Cleaned_Images\"\n",
    "train_pseudo_mask_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Training_Pseudo_Masks\"\n",
    "groundtruth = \"E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Training_GroundTruth_Transformed.csv\"\n",
    "\n",
    "# Image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = {\n",
    "    'NV': 0, 'MEL': 1, 'BCC': 2, 'BKL': 3,\n",
    "    'AK': 4, 'SCC': 5, 'VASC': 6, 'DF': 7\n",
    "}\n",
    "\n",
    "# Verify paths\n",
    "for path in [groundtruth, train_image_dir, train_pseudo_mask_dir]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"The path {path} does not exist. Please check and update the path.\")\n",
    "\n",
    "# Custom Dataset for SMOTE\n",
    "class ISIC2019FeatureDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, pseudo_mask_dir):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.pseudo_mask_dir = pseudo_mask_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx][\"image\"]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n",
    "        mask_path = os.path.join(self.pseudo_mask_dir, f\"{img_name}.png\")\n",
    "        label_str = self.dataframe.iloc[idx][\"diagnosis\"]\n",
    "        label = label_mapping[label_str]\n",
    "\n",
    "        # Verify image and mask exist\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image {img_path} not found. Skipping.\")\n",
    "            return None\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"Warning: Mask {mask_path} not found. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        # Apply resize transform\n",
    "        image = self.transform(image)\n",
    "        mask = transforms.Resize((IMG_SIZE, IMG_SIZE))(mask)\n",
    "        mask = np.array(mask, dtype=np.float32) / 255.0\n",
    "\n",
    "        # Flatten image features (RGB + mask)\n",
    "        image = image.numpy().flatten()\n",
    "        mask = mask.flatten()\n",
    "        features = np.concatenate([image, mask])\n",
    "\n",
    "        return features, label\n",
    "\n",
    "# Load ground truth\n",
    "try:\n",
    "    df = pd.read_csv(groundtruth)\n",
    "    print(\"Ground truth loaded successfully. Columns:\", df.columns.tolist())\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to load CSV at {groundtruth}: {str(e)}\")\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = [\"image\", \"diagnosis\"]\n",
    "if not all(col in df.columns for col in required_columns):\n",
    "    raise ValueError(f\"CSV must contain columns: {required_columns}. Found: {df.columns.tolist()}\")\n",
    "\n",
    "# Create dataset for feature extraction\n",
    "feature_dataset = ISIC2019FeatureDataset(df, train_image_dir, train_pseudo_mask_dir)\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Extract features and labels with tqdm progress bar\n",
    "for item in tqdm(feature_dataset, desc=\"Extracting features\"):\n",
    "    if item is None:  # Skip invalid entries\n",
    "        continue\n",
    "    feature, label = item\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(features, labels)\n",
    "\n",
    "# Create new DataFrame for resampled data\n",
    "resampled_data = []\n",
    "for i, (feature, label) in enumerate(zip(X_resampled, y_resampled)):\n",
    "    img_name = f\"synthetic_{i}\"\n",
    "    label_str = [k for k, v in label_mapping.items() if v == label][0]\n",
    "    resampled_data.append({\"image\": img_name, \"diagnosis\": label_str})\n",
    "\n",
    "resampled_df = pd.DataFrame(resampled_data)\n",
    "\n",
    "# Save resampled DataFrame\n",
    "output_path = \"E:/Code/ClassPlusSeg/Final_Dataset/ISIC_2019_Training_GroundTruth_SMOTE.csv\"\n",
    "resampled_df.to_csv(output_path, index=False)\n",
    "print(f\"SMOTE-balanced CSV saved to: {output_path}\")\n",
    "\n",
    "# Print class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(resampled_df['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94cec0-1a7b-4bb4-aff6-439adbf23497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights, densenet169, DenseNet169_Weights\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define SE (Squeeze-and-Excitation) Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# Define CBAM (Convolutional Block Attention Module) + SE Block\n",
    "class CBAM_SE(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(CBAM_SE, self).__init__()\n",
    "        # Channel Attention (CBAM)\n",
    "        self.channel_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.channel_mlp = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False)\n",
    "        )\n",
    "        self.channel_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Spatial Attention (CBAM)\n",
    "        self.spatial_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.spatial_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # SE Block\n",
    "        self.se = SEBlock(channels, reduction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel Attention (CBAM)\n",
    "        avg_pool = self.channel_avg_pool(x).view(x.size(0), x.size(1))\n",
    "        max_pool = self.channel_max_pool(x).view(x.size(0), x.size(1))\n",
    "        avg_out = self.channel_mlp(avg_pool)\n",
    "        max_out = self.channel_mlp(max_pool)\n",
    "        channel_att = self.channel_sigmoid(avg_out + max_out).view(x.size(0), x.size(1), 1, 1)\n",
    "        x = x * channel_att\n",
    "\n",
    "        # Spatial Attention (CBAM)\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_input = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        spatial_att = self.spatial_sigmoid(self.spatial_conv(spatial_input))\n",
    "        x = x * spatial_att\n",
    "\n",
    "        # Apply SE Block\n",
    "        x = self.se(x)\n",
    "        return x\n",
    "\n",
    "# Define the Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "\n",
    "        # Load Pretrained models\n",
    "        self.efficientnet = efficientnet_b4(weights=EfficientNet_B4_Weights.IMAGENET1K_V1)\n",
    "        self.densenet = densenet169(weights=DenseNet169_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Extract feature extractor part\n",
    "        self.efficientnet = self.efficientnet.features\n",
    "        self.densenet = self.densenet.features\n",
    "\n",
    "        # Pooling & Flattening\n",
    "        self.pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # CBAM + SE Attention after concatenation\n",
    "        self.cbam_se = CBAM_SE(1792 + 1664)\n",
    "\n",
    "        # Fully Connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1792 + 1664, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        feat_a = self.efficientnet(x)\n",
    "        feat_b = self.densenet(x)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined = torch.cat((feat_a, feat_b), dim=1)\n",
    "\n",
    "        # Apply CBAM + SE attention\n",
    "        combined = self.cbam_se(combined)\n",
    "\n",
    "        # Max Pooling and Flatten\n",
    "        combined = self.pool(combined)\n",
    "        combined = self.flatten(combined)\n",
    "\n",
    "        # Fully connected layers\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "\n",
    "# Feature Extractor for SMOTE\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.efficientnet = efficientnet_b4(weights=EfficientNet_B4_Weights.IMAGENET1K_V1).features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.efficientnet(x)\n",
    "        features = self.pool(features)\n",
    "        features = self.flatten(features)\n",
    "        return features\n",
    "\n",
    "# Label mapping for multiclass classification\n",
    "label_mapping = {\n",
    "    \"MEL\": 0,  # Melanoma\n",
    "    \"NV\": 1,   # Melanocytic Nevus\n",
    "    \"BCC\": 2,  # Basal Cell Carcinoma\n",
    "    \"AK\": 3,   # Actinic Keratosis\n",
    "    \"BKL\": 4,  # Benign Keratosis\n",
    "    \"DF\": 5,   # Dermatofibroma\n",
    "    \"VASC\": 6, # Vascular Lesion\n",
    "    \"SCC\": 7   # Squamous Cell Carcinoma\n",
    "}\n",
    "\n",
    "# Custom Dataset for SMOTE Feature Extraction\n",
    "class ISIC2019FeatureDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx][\"image\"]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n",
    "        label_str = self.dataframe.iloc[idx][\"diagnosis\"]\n",
    "        label = label_mapping[label_str]\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image {img_path} not found. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Dataset class for training/validation/test\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, synthetic_image_folder=None, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.synthetic_image_folder = synthetic_image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx][\"image\"]\n",
    "        label_str = self.dataframe.iloc[idx][\"diagnosis\"]\n",
    "        label = label_mapping[label_str]\n",
    "\n",
    "        # Determine image path (synthetic or original)\n",
    "        if self.synthetic_image_folder and \"synthetic\" in img_name:\n",
    "            img_path = os.path.join(self.synthetic_image_folder, f\"{img_name}.jpg\")\n",
    "        else:\n",
    "            img_path = os.path.join(self.image_folder, f\"{img_name}.jpg\")\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Image {img_path} not found.\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Load training dataset\n",
    "train_csv_path = r\"E:\\Code\\ClassPlusSeg\\Final_Dataset\\ISIC_2019_Training_GroundTruth_Transformed.csv\"\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(\"Training dataset loaded. Columns:\", train_df.columns.tolist())\n",
    "\n",
    "# Split training data into train (80%) and validation (20%)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"diagnosis\"])\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n",
    "\n",
    "# Load test dataset\n",
    "test_csv_path = r\"E:\\Code\\ClassPlusSeg\\Final_Dataset\\ISIC_2019_Testing_GroundTruth_Transformed_Simplified.csv\"\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print(f\"Test dataset loaded. Columns: {test_df.columns.tolist()}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Extract features for SMOTE using EfficientNet-B4\n",
    "image_dir = r\"E:\\Code\\ClassPlusSeg\\Final_Dataset\\ISIC_2019_Training_Input_Cleaned_Images\"\n",
    "feature_dataset = ISIC2019FeatureDataset(train_df, image_dir)\n",
    "feature_loader = DataLoader(feature_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, lbls in tqdm(feature_loader, desc=\"Extracting features for SMOTE\"):\n",
    "        if images is None:\n",
    "            continue\n",
    "        images = images.to(device)\n",
    "        feats = feature_extractor(images)\n",
    "        features.append(feats.cpu().numpy())\n",
    "        labels.append(lbls.numpy())\n",
    "\n",
    "features = np.vstack(features)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(features, labels)\n",
    "\n",
    "# Create synthetic DataFrame (map to original images for simplicity)\n",
    "resampled_data = []\n",
    "original_images = train_df[\"image\"].values\n",
    "original_labels = train_df[\"diagnosis\"].values\n",
    "label_to_images = {lbl: [] for lbl in label_mapping.keys()}\n",
    "for img, lbl in zip(original_images, original_labels):\n",
    "    label_to_images[lbl].append(img)\n",
    "\n",
    "for i, label in enumerate(tqdm(y_resampled, desc=\"Creating synthetic DataFrame\")):\n",
    "    label_str = [k for k, v in label_mapping.items() if v == label][0]\n",
    "    # Randomly select an original image from the same class\n",
    "    img_name = np.random.choice(label_to_images[label_str])\n",
    "    resampled_data.append({\"image\": img_name, \"diagnosis\": label_str})\n",
    "\n",
    "train_smote_df = pd.DataFrame(resampled_data)\n",
    "print(\"SMOTE applied to training data. New train size:\", len(train_smote_df))\n",
    "print(\"Training class distribution after SMOTE:\")\n",
    "print(train_smote_df['diagnosis'].value_counts())\n",
    "\n",
    "# Image Augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test Transform (no augmentation)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_image_folder = r\"E:\\Code\\ClassPlusSeg\\Final_Dataset\\ISIC_2019_Training_Input_Cleaned_Images\"\n",
    "test_image_folder = r\"E:\\Code\\ClassPlusSeg\\Final_Dataset\\ISIC_2019_Testing_Input_Cleaned_Images\"\n",
    "synthetic_image_folder = None  # Not using synthetic images\n",
    "train_dataset = SkinLesionDataset(train_smote_df, train_image_folder, synthetic_image_folder, train_transform)\n",
    "val_dataset = SkinLesionDataset(val_df, train_image_folder, transform=val_test_transform)\n",
    "test_dataset = SkinLesionDataset(test_df, test_image_folder, transform=val_test_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "print(f\"Train DataLoader size: {len(train_loader.dataset)}, Validation DataLoader size: {len(val_loader.dataset)}, Test DataLoader size: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(label_mapping)\n",
    "model = EnsembleModel(num_classes=num_classes).to(device)\n",
    "print(\"Model initialized and moved to GPU.\")\n",
    "\n",
    "# Loss, Optimizer, and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, loader, criterion, device, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=desc):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    ms_per_step = (elapsed_time / len(loader)) * 1000  # ms/step\n",
    "    return (total_loss / len(loader), 100 * correct / total, ms_per_step, np/array(all_preds), np.array(all_labels))\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, device, epochs=50, patience=5, save_path='best_ensemble_model.pth'):\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_path = save_path\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0, 0, 0\n",
    "        start_time = time.time()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        train_time = time.time() - start_time\n",
    "        train_ms_per_step = (train_time / len(train_loader)) * 1000  # ms/step\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # Evaluate on validation and test sets\n",
    "        val_loss, val_acc, val_ms_per_step, _, _ = evaluate(model, val_loader, criterion, device, desc=\"Validating\")\n",
    "        test_loss, test_acc, test_ms_per_step, test_preds, test_labels = evaluate(model, test_loader, criterion, device, desc=\"Testing\")\n",
    "\n",
    "        # Compute absolute variance\n",
    "        abs_variance = abs(test_acc - val_acc)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "        print(f\"Absolute Variance (Test - Validation Acc): {abs_variance:.2f}%\")\n",
    "        print(f\"Time per Step (ms/step) - Train: {train_ms_per_step:.2f}, Val: {val_ms_per_step:.2f}, Test: {test_ms_per_step:.2f}\")\n",
    "        print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(test_labels, test_preds)\n",
    "        print(\"Test Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, best_model_path)\n",
    "            print(f\"New best model saved with Val Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    # Load and evaluate best model\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\\nLoaded best model with Val Acc: {checkpoint['val_acc']:.2f}%\")\n",
    "\n",
    "    # Final evaluation\n",
    "    final_train_loss, final_train_acc, final_train_ms, _, _ = evaluate(model, train_loader, criterion, device, desc=\"Final Train Eval\")\n",
    "    final_val_loss, final_val_acc, final_val_ms, _, _ = evaluate(model, val_loader, criterion, device, desc=\"Final Val Eval\")\n",
    "    final_test_loss, final_test_acc, final_test_ms, final_test_preds, final_test_labels = evaluate(model, test_loader, criterion, device, desc=\"Final Test Eval\")\n",
    "    final_abs_variance = abs(final_test_acc - final_val_acc)\n",
    "    final_cm = confusion_matrix(final_test_labels, final_test_preds)\n",
    "\n",
    "    print(\"\\nFinal Metrics (Best Model):\")\n",
    "    print(f\"Train Loss: {final_train_loss:.4f}, Train Acc: {final_train_acc:.2f}%\")\n",
    "    print(f\"Validation Loss: {final_val_loss:.4f}, Validation Acc: {final_val_acc:.2f}%\")\n",
    "    print(f\"Test Loss: {final_test_loss:.4f}, Test Acc: {final_test_acc:.2f}%\")\n",
    "    print(f\"Absolute Variance (Test - Validation Acc): {final_abs_variance:.2f}%\")\n",
    "    print(f\"Time per Step (ms/step) - Train: {final_train_ms:.2f}, Val: {final_val_ms:.2f}, Test: {final_test_ms:.2f}\")\n",
    "    print(\"Final Test Confusion Matrix:\")\n",
    "    print(final_cm)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Run training\n",
    "def main():\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        epochs=50,\n",
    "        patience=5,\n",
    "        save_path='best_ensemble_model_low_memory.pth'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f699b1-54ad-461c-92dd-8e717af5493d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09cdb2b-5e76-46dc-b462-b5a4bd9e3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu118\n",
      "CUDA Available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eda911e-a6c1-4589-95d1-ea74a037809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: cleaned_ISIC_0012255.jpg -> ISIC_0012255.jpg\n",
      "Processed: cleaned_ISIC_0012346.jpg -> ISIC_0012346.jpg\n",
      "Processed: cleaned_ISIC_0012576.jpg -> ISIC_0012576.jpg\n",
      "Processed: cleaned_ISIC_0012585.jpg -> ISIC_0012585.jpg\n",
      "Processed: cleaned_ISIC_0012623.jpg -> ISIC_0012623.jpg\n",
      "Processed: cleaned_ISIC_0012627.jpg -> ISIC_0012627.jpg\n",
      "Processed: cleaned_ISIC_0012633.jpg -> ISIC_0012633.jpg\n",
      "Processed: cleaned_ISIC_0012643.jpg -> ISIC_0012643.jpg\n",
      "Processed: cleaned_ISIC_0015294.jpg -> ISIC_0015294.jpg\n",
      "Processed: cleaned_ISIC_0015351.jpg -> ISIC_0015351.jpg\n",
      "Processed: cleaned_ISIC_0015370.jpg -> ISIC_0015370.jpg\n",
      "Processed: cleaned_ISIC_0015462.jpg -> ISIC_0015462.jpg\n",
      "Processed: cleaned_ISIC_0015480.jpg -> ISIC_0015480.jpg\n",
      "Processed: cleaned_ISIC_0015492.jpg -> ISIC_0015492.jpg\n",
      "Processed: cleaned_ISIC_0015518.jpg -> ISIC_0015518.jpg\n",
      "Processed: cleaned_ISIC_0015552.jpg -> ISIC_0015552.jpg\n",
      "Processed: cleaned_ISIC_0015590.jpg -> ISIC_0015590.jpg\n",
      "Processed: cleaned_ISIC_0015634.jpg -> ISIC_0015634.jpg\n",
      "Processed: cleaned_ISIC_0016351.jpg -> ISIC_0016351.jpg\n",
      "Processed: cleaned_ISIC_0016714.jpg -> ISIC_0016714.jpg\n",
      "Processed: cleaned_ISIC_0016804.jpg -> ISIC_0016804.jpg\n",
      "Processed: cleaned_ISIC_0017341.jpg -> ISIC_0017341.jpg\n",
      "Processed: cleaned_ISIC_0017398.jpg -> ISIC_0017398.jpg\n",
      "Processed: cleaned_ISIC_0017399.jpg -> ISIC_0017399.jpg\n",
      "Processed: cleaned_ISIC_0017460.jpg -> ISIC_0017460.jpg\n",
      "Processed: cleaned_ISIC_0017474.jpg -> ISIC_0017474.jpg\n",
      "Processed: cleaned_ISIC_0017702.jpg -> ISIC_0017702.jpg\n",
      "Processed: cleaned_ISIC_0017755.jpg -> ISIC_0017755.jpg\n",
      "Processed: cleaned_ISIC_0018111.jpg -> ISIC_0018111.jpg\n",
      "Processed: cleaned_ISIC_0018179.jpg -> ISIC_0018179.jpg\n",
      "Processed: cleaned_ISIC_0018248.jpg -> ISIC_0018248.jpg\n",
      "Processed: cleaned_ISIC_0018375.jpg -> ISIC_0018375.jpg\n",
      "Processed: cleaned_ISIC_0018472.jpg -> ISIC_0018472.jpg\n",
      "Processed: cleaned_ISIC_0018521.jpg -> ISIC_0018521.jpg\n",
      "Processed: cleaned_ISIC_0018556.jpg -> ISIC_0018556.jpg\n",
      "Processed: cleaned_ISIC_0018611.jpg -> ISIC_0018611.jpg\n",
      "Processed: cleaned_ISIC_0018680.jpg -> ISIC_0018680.jpg\n",
      "Processed: cleaned_ISIC_0019049.jpg -> ISIC_0019049.jpg\n",
      "Processed: cleaned_ISIC_0019309.jpg -> ISIC_0019309.jpg\n",
      "Processed: cleaned_ISIC_0019334.jpg -> ISIC_0019334.jpg\n",
      "Processed: cleaned_ISIC_0019723.jpg -> ISIC_0019723.jpg\n",
      "Processed: cleaned_ISIC_0019794.jpg -> ISIC_0019794.jpg\n",
      "Processed: cleaned_ISIC_0019883.jpg -> ISIC_0019883.jpg\n",
      "Processed: cleaned_ISIC_0020233.jpg -> ISIC_0020233.jpg\n",
      "Processed: cleaned_ISIC_0020418.jpg -> ISIC_0020418.jpg\n",
      "Processed: cleaned_ISIC_0020861.jpg -> ISIC_0020861.jpg\n",
      "Processed: cleaned_ISIC_0020893.jpg -> ISIC_0020893.jpg\n",
      "Processed: cleaned_ISIC_0020953.jpg -> ISIC_0020953.jpg\n",
      "Processed: cleaned_ISIC_0020999.jpg -> ISIC_0020999.jpg\n",
      "Processed: cleaned_ISIC_0021037.jpg -> ISIC_0021037.jpg\n",
      "Processed: cleaned_ISIC_0021041.jpg -> ISIC_0021041.jpg\n",
      "Processed: cleaned_ISIC_0021152.jpg -> ISIC_0021152.jpg\n",
      "Processed: cleaned_ISIC_0021158.jpg -> ISIC_0021158.jpg\n",
      "Processed: cleaned_ISIC_0021202.jpg -> ISIC_0021202.jpg\n",
      "Processed: cleaned_ISIC_0021251.jpg -> ISIC_0021251.jpg\n",
      "Processed: cleaned_ISIC_0021448.jpg -> ISIC_0021448.jpg\n",
      "Processed: cleaned_ISIC_0021449.jpg -> ISIC_0021449.jpg\n",
      "Processed: cleaned_ISIC_0021504.jpg -> ISIC_0021504.jpg\n",
      "Processed: cleaned_ISIC_0021714.jpg -> ISIC_0021714.jpg\n",
      "Processed: cleaned_ISIC_0021762.jpg -> ISIC_0021762.jpg\n",
      "Processed: cleaned_ISIC_0021816.jpg -> ISIC_0021816.jpg\n",
      "Processed: cleaned_ISIC_0021904.jpg -> ISIC_0021904.jpg\n",
      "Processed: cleaned_ISIC_0021914.jpg -> ISIC_0021914.jpg\n",
      "Processed: cleaned_ISIC_0021990.jpg -> ISIC_0021990.jpg\n",
      "Processed: cleaned_ISIC_0022029.jpg -> ISIC_0022029.jpg\n",
      "Processed: cleaned_ISIC_0022039.jpg -> ISIC_0022039.jpg\n",
      "Processed: cleaned_ISIC_0022147.jpg -> ISIC_0022147.jpg\n",
      "Processed: cleaned_ISIC_0022192.jpg -> ISIC_0022192.jpg\n",
      "Processed: cleaned_ISIC_0022221.jpg -> ISIC_0022221.jpg\n",
      "Processed: cleaned_ISIC_0022657.jpg -> ISIC_0022657.jpg\n",
      "Processed: cleaned_ISIC_0022738.jpg -> ISIC_0022738.jpg\n",
      "Processed: cleaned_ISIC_0023371.jpg -> ISIC_0023371.jpg\n",
      "Processed: cleaned_ISIC_0023508.jpg -> ISIC_0023508.jpg\n",
      "Processed: cleaned_ISIC_0023628.jpg -> ISIC_0023628.jpg\n",
      "Processed: cleaned_ISIC_0023678.jpg -> ISIC_0023678.jpg\n",
      "Processed: cleaned_ISIC_0023755.jpg -> ISIC_0023755.jpg\n",
      "Processed: cleaned_ISIC_0023831.jpg -> ISIC_0023831.jpg\n",
      "Processed: cleaned_ISIC_0023900.jpg -> ISIC_0023900.jpg\n",
      "Processed: cleaned_ISIC_0023904.jpg -> ISIC_0023904.jpg\n",
      "Processed: cleaned_ISIC_0023924.jpg -> ISIC_0023924.jpg\n",
      "Processed: cleaned_ISIC_0023936.jpg -> ISIC_0023936.jpg\n",
      "Processed: cleaned_ISIC_0024135.jpg -> ISIC_0024135.jpg\n",
      "Processed: cleaned_ISIC_0036073.jpg -> ISIC_0036073.jpg\n",
      "Processed: cleaned_ISIC_0036085.jpg -> ISIC_0036085.jpg\n",
      "Processed: cleaned_ISIC_0036098.jpg -> ISIC_0036098.jpg\n",
      "Processed: cleaned_ISIC_0036101.jpg -> ISIC_0036101.jpg\n",
      "Processed: cleaned_ISIC_0036121.jpg -> ISIC_0036121.jpg\n",
      "Processed: cleaned_ISIC_0036147.jpg -> ISIC_0036147.jpg\n",
      "Processed: cleaned_ISIC_0036174.jpg -> ISIC_0036174.jpg\n",
      "Processed: cleaned_ISIC_0036206.jpg -> ISIC_0036206.jpg\n",
      "Processed: cleaned_ISIC_0036236.jpg -> ISIC_0036236.jpg\n",
      "Processed: cleaned_ISIC_0036237.jpg -> ISIC_0036237.jpg\n",
      "Processed: cleaned_ISIC_0036240.jpg -> ISIC_0036240.jpg\n",
      "Processed: cleaned_ISIC_0036247.jpg -> ISIC_0036247.jpg\n",
      "Processed: cleaned_ISIC_0036281.jpg -> ISIC_0036281.jpg\n",
      "Processed: cleaned_ISIC_0036291.jpg -> ISIC_0036291.jpg\n",
      "Processed: cleaned_ISIC_0036306.jpg -> ISIC_0036306.jpg\n",
      "Processed: cleaned_ISIC_0036321.jpg -> ISIC_0036321.jpg\n",
      "Processed: cleaned_ISIC_0036328.jpg -> ISIC_0036328.jpg\n",
      "Processed: cleaned_ISIC_0036333.jpg -> ISIC_0036333.jpg\n",
      "All files processed and saved to E:/Code/ClassPlusSeg/Final_Dataset/Isic_2018_validation_Images_Cleaned_v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "input_dir = \"E:/Code/ClassPlusSeg/ISIC2018_Task1-2_Validation_Input_Cleaned\"\n",
    "output_dir = \"E:/Code/ClassPlusSeg/Final_Dataset/Isic_2018_validation_Images_Cleaned_v1\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all files in the input directory\n",
    "file_paths = sorted(glob.glob(os.path.join(input_dir, \"*.jpg\")) + \n",
    "                    glob.glob(os.path.join(input_dir, \"*.png\")))\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Extract the filename from the full path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    # Check if the filename contains \"cleaned_\"\n",
    "    if \"cleaned_\" in file_name:\n",
    "        # Remove \"cleaned_\" from the filename\n",
    "        new_file_name = file_name.replace(\"cleaned_\", \"\")\n",
    "        \n",
    "        # Define the new file path in the output directory\n",
    "        new_file_path = os.path.join(output_dir, new_file_name)\n",
    "        \n",
    "        # Copy the file to the new location with the updated name\n",
    "        shutil.copy(file_path, new_file_path)\n",
    "        print(f\"Processed: {file_name} -> {new_file_name}\")\n",
    "\n",
    "print(f\"All files processed and saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a496b0-24bc-49bc-b838-f3f9379a6058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: Segmentation_Results\n",
      "Train: 2594 - 2594\n",
      "Valid: 100 - 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 283\u001b[0m\n\u001b[0;32m    281\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    282\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[0;32m    284\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    285\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 239\u001b[0m, in \u001b[0;36mISICDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m    238\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]\n\u001b[1;32m--> 239\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[0;32m    240\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (W, H))\n\u001b[0;32m    241\u001b[0m     img \u001b[38;5;241m=\u001b[39m img \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "\n",
    "# Data directories (only train and validation)\n",
    "train_image_dir = \"E:/Code/ClassPlusSeg/ISIC2018_Task1-2_Training_Input_Cleaned\"\n",
    "train_mask_dir = \"E:/Code/ClassPlusSeg/ISIC2018_Task1_Training_GroundTruth\"\n",
    "val_image_dir = \"E:/Code/ClassPlusSeg/ISIC2018_Task1-2_Validation_Input_Cleaned\"\n",
    "val_mask_dir = \"E:/Code/ClassPlusSeg/ISIC2018_Task1_Validation_GroundTruth\"\n",
    "\n",
    "# Global parameters\n",
    "H = 256\n",
    "W = 256\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# SE Block Definition\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# BatchNormReLU\n",
    "class BatchNormReLU(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(BatchNormReLU, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Residual Block with SE\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, strides=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=strides, padding=1, bias=False)\n",
    "        self.bn_relu1 = BatchNormReLU(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_relu2 = BatchNormReLU(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if strides != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=strides, padding=0, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn_relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn_relu2(x)\n",
    "        x = self.se(x)\n",
    "        x = x + identity\n",
    "        return x\n",
    "\n",
    "# Decoder Block with SE\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.concat_channels = in_channels + skip_channels\n",
    "        self.res_block = ResidualBlock(self.concat_channels, out_channels, strides=1)\n",
    "        self.se = SEBlock(out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res_block(x)\n",
    "        x = self.se(x)\n",
    "        return x\n",
    "\n",
    "# Dense PPM Bridge\n",
    "class DensePPMBridge(nn.Module):\n",
    "    def __init__(self, in_channels=256, out_channels=512):\n",
    "        super(DensePPMBridge, self).__init__()\n",
    "        growth_rate = 128\n",
    "\n",
    "        self.dense1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            BatchNormReLU(growth_rate)\n",
    "        )\n",
    "        self.dense2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            BatchNormReLU(growth_rate)\n",
    "        )\n",
    "\n",
    "        self.ppm_pool1 = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        self.ppm_pool2 = nn.Sequential(nn.AdaptiveAvgPool2d(2), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        self.ppm_pool3 = nn.Sequential(nn.AdaptiveAvgPool2d(3), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        self.ppm_pool6 = nn.Sequential(nn.AdaptiveAvgPool2d(6), nn.Conv2d(in_channels + 2 * growth_rate, 128, 1, bias=False), BatchNormReLU(128))\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels + 2 * growth_rate + 4 * 128, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.dense1(x)\n",
    "        d1_cat = torch.cat([F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=True), d1], dim=1)\n",
    "        d2 = self.dense2(d1_cat)\n",
    "        dense_out = torch.cat([F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=True), d1, d2], dim=1)\n",
    "\n",
    "        ppm1 = self.ppm_pool1(dense_out)\n",
    "        ppm1 = F.interpolate(ppm1, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "        ppm2 = self.ppm_pool2(dense_out)\n",
    "        ppm2 = F.interpolate(ppm2, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "        ppm3 = self.ppm_pool3(dense_out)\n",
    "        ppm3 = F.interpolate(ppm3, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "        ppm6 = self.ppm_pool6(dense_out)\n",
    "        ppm6 = F.interpolate(ppm6, size=dense_out.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        ppm_out = torch.cat([dense_out, ppm1, ppm2, ppm3, ppm6], dim=1)\n",
    "        out = self.final_conv(ppm_out)\n",
    "        return out\n",
    "\n",
    "# SE-ResUNet Model\n",
    "class SEResUNet(nn.Module):\n",
    "    def __init__(self, input_shape=(256, 256, 3)):\n",
    "        super(SEResUNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_relu1 = BatchNormReLU(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.se1 = SEBlock(64)\n",
    "        self.shortcut1 = nn.Conv2d(3, 64, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        self.encoder2 = ResidualBlock(64, 128, strides=2)\n",
    "        self.encoder3 = ResidualBlock(128, 256, strides=2)\n",
    "        self.bridge = DensePPMBridge(256, 512)\n",
    "\n",
    "        self.decoder1 = DecoderBlock(512, 256, 256)\n",
    "        self.decoder2 = DecoderBlock(256, 128, 128)\n",
    "        self.decoder3 = DecoderBlock(128, 64, 64)\n",
    "\n",
    "        self.output = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1 = self.conv1(x)\n",
    "        s1 = self.bn_relu1(s1)\n",
    "        s1 = self.conv2(s1)\n",
    "        s1 = self.se1(s1)\n",
    "        shortcut = self.shortcut1(x)\n",
    "        s1 = s1 + shortcut\n",
    "\n",
    "        s2 = self.encoder2(s1)\n",
    "        s3 = self.encoder3(s2)\n",
    "        b = self.bridge(s3)\n",
    "\n",
    "        d1 = self.decoder1(b, s3)\n",
    "        d2 = self.decoder2(d1, s2)\n",
    "        d3 = self.decoder3(d2, s1)\n",
    "\n",
    "        out = self.output(d3)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Metrics and Utility Functions\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-15):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = y_pred.view(-1)\n",
    "        y_true = y_true.view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
    "        return 1.0 - dice\n",
    "\n",
    "def dice_coef(y_pred, y_true, smooth=1e-15):\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_true = y_true.view(-1)\n",
    "    intersection = (y_pred * y_true).sum()\n",
    "    return (2. * intersection + smooth) / (y_pred.sum() + y_true.sum() + smooth)\n",
    "\n",
    "def iou(y_pred, y_true, smooth=1e-15):\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_true = y_true.view(-1)\n",
    "    intersection = (y_pred * y_true).sum()\n",
    "    union = y_true.sum() + y_pred.sum() - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "def create_dir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(f\"Created directory: {path}\")\n",
    "        else:\n",
    "            print(f\"Directory already exists: {path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_data():\n",
    "    train_x = sorted(glob(os.path.join(train_image_dir, \"*.jpg\")))\n",
    "    train_y = sorted(glob(os.path.join(train_mask_dir, \"*.png\")))\n",
    "    valid_x = sorted(glob(os.path.join(val_image_dir, \"*.jpg\")))\n",
    "    valid_y = sorted(glob(os.path.join(val_mask_dir, \"*.png\")))\n",
    "    return (train_x, train_y), (valid_x, valid_y)\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img, (W, H))\n",
    "        img = img / 255.0\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        mask_path = self.masks[idx]\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, (W, H))\n",
    "        mask = mask / 255.0\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        return torch.tensor(img, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "# Main Execution\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "create_dir(\"Segmentation_Results\")\n",
    "\n",
    "(train_x, train_y), (valid_x, valid_y) = load_data()\n",
    "\n",
    "print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
    "\n",
    "train_dataset = ISICDataset(train_x, train_y)\n",
    "valid_dataset = ISICDataset(valid_x, valid_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = SEResUNet(input_shape=(H, W, 3)).to(device)\n",
    "criterion = DiceLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "model_path = os.path.join(\"files\", \"segModel.pth\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "    for images, masks in train_bar:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        train_bar.set_postfix({\"Train Loss\": loss.item()})\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_dice = 0.0\n",
    "    val_iou = 0.0\n",
    "    val_bar = tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_bar:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            outputs = (outputs > 0.5).float()\n",
    "            val_dice += dice_coef(outputs, masks).item() * images.size(0)\n",
    "            val_iou += iou(outputs, masks).item() * images.size(0)\n",
    "            val_bar.set_postfix({\"Val Loss\": loss.item()})\n",
    "    \n",
    "    val_loss /= len(valid_loader.dataset)\n",
    "    val_dice /= len(valid_loader.dataset)\n",
    "    val_iou /= len(valid_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val IoU: {val_iou:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        create_dir(\"files\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6194b30-512b-4364-b373-445cc7c0b98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
